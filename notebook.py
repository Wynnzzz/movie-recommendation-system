# -*- coding: utf-8 -*-
"""Copy of notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dLBrqVAnzRjgaatgwSW7mSSbxJxw7CmT

# Loading Data
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

df_movies=pd.read_csv('/content/drive/MyDrive/Submission Recommendation System/data/movies.csv')
df_ratings=pd.read_csv('/content/drive/MyDrive/Submission Recommendation System/data/ratings.csv')

df_movies.head()

df_ratings.head()

df_movies.info()

df_ratings.info()

"""# Exploratory Data Analysis

## Movies
"""

df_movies.isna().sum()

df_movies.duplicated().sum()

print('Jumlah Film:', len(df_movies.movieId.unique()))
print('Jumlah Judul Film:', len(df_movies.title.unique()))
print('Jumlah Genre:', len(df_movies.genres.unique()))

"""## df_ratings"""

df_ratings.isna().sum()

df_ratings.duplicated().sum()

print('Jumlah userId: ', len(df_ratings.userId.unique()))
print('Jumlah film yang diulas: ', len(df_ratings.movieId.unique()))
print('Jumlah data rating: ', len(df_ratings))

"""# Data Preprocessing"""

df_movies['genres']=df_movies['genres'].str.replace('|', ' ')

import re

def clean_title(text):
    text = text.lower()  # lowercase
    text = re.sub(r'\([^)]*\)', '', text)  # hapus tanda kurung dan isinya, misal "(1995)"
    text = re.sub(r'[^a-z0-9\s]', '', text)  # hapus karakter selain huruf, angka, dan spasi
    text = re.sub(r'\s+', ' ', text).strip()  # hapus spasi berlebih
    return text

df_movies['clean_title'] = df_movies['title'].apply(clean_title)

df_movies.head()

df_movies['genres'].eq('(no genres listed)').sum()

df_movies= df_movies[df_movies['genres'] != '(no genres listed)']

df_movies['genres_list'] = df_movies['genres']

list_genre=pd.Series([genre for genres_list in df_movies['genres'].str.split() for genre in genres_list]).unique()
list_genre

print('Jumlah jenis genre:', len(list_genre))

import pandas as pd
import matplotlib.pyplot as plt

# Hitung jumlah film per genre dari df_movies_cleaned
genre_counts = pd.Series(
    [genre for genres_list in df_movies['genres'].str.split() for genre in genres_list]
).value_counts()

# Plot
plt.figure(figsize=(12, 6))
genre_counts.plot(kind='bar', color='green')
plt.title('Jumlah Film per Genre', fontsize=16)
plt.xlabel('Genre', fontsize=14)
plt.ylabel('Jumlah Film', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""## df_ratings"""

df_ratings=df_ratings.drop(columns='timestamp', axis=1)

df_ratings.head()

combined_data = pd.merge(df_ratings, df_movies, on='movieId', how='inner')

combined_data.head()

# Menghitung rata-rata rating per film
rating_rata2 = combined_data.groupby('title').agg({'rating': 'mean'})

# Urutkan dari yang tertinggi dan ambil 10 teratas
top_10_rating = rating_rata2.sort_values(by='rating', ascending=False).head(10)

print("Top 10 Film dengan Rata-Rata Rating Tertinggi:")
print(top_10_rating)

user_ids = df_ratings['userId'].unique().tolist()
movie_ids = df_ratings['movieId'].unique().tolist()

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

df_ratings['user'] = df_ratings['userId'].map(user_to_user_encoded)
df_ratings['movie'] = df_ratings['movieId'].map(movie_to_movie_encoded)

num_users = len(user_ids)
num_movies = len(movie_ids)
min_rating = df_ratings['rating'].min()
max_rating = df_ratings['rating'].max()

df_ratings = df_ratings.sample(frac=1, random_state=42)
x = df_ratings[['user', 'movie']].values
y = df_ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

x = df_ratings[['user', 'movie']].values
y = df_ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * len(x))
x_train, x_val = x[:train_indices], x[train_indices:]
y_train, y_val = y[:train_indices], y[train_indices:]

"""# Recommendation"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_embedding = layers.Embedding(num_movies, embedding_size,
                                                embeddings_initializer='he_normal',
                                                embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = tf.reduce_sum(user_vector * movie_vector, axis=1, keepdims=True)
        x = dot_user_movie + user_bias + movie_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_movies, 50)
model.compile(loss='binary_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=1e-4),
              metrics=[tf.keras.metrics.RootMeanSquaredError()])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

callbacks = [
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1),
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

history = model.fit(
    x_train, y_train,
    batch_size=1024,
    validation_data=(x_val, y_val),
    epochs=20,
    callbacks=callbacks
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.legend(["Train", "Val"])
plt.show()

user_id = df_ratings['userId'].sample(1).iloc[0]
encoded_user_id = user_to_user_encoded[user_id]

movies_watched = df_ratings[df_ratings['userId'] == user_id]['movieId'].tolist()
movies_not_watched = [mid for mid in movie_ids if mid not in movies_watched]
movies_not_watched_encoded = [movie_to_movie_encoded[mid] for mid in movies_not_watched]

user_movie_array = np.hstack(
    ([[encoded_user_id]] * len(movies_not_watched_encoded), np.array(movies_not_watched_encoded).reshape(-1, 1))
)

ratings_pred = model.predict(user_movie_array).flatten()
top_indices = ratings_pred.argsort()[-10:][::-1]

recommended_movie_ids = [movie_encoded_to_movie[movies_not_watched_encoded[i]] for i in top_indices]
recommended_movies = df_movies[df_movies['movieId'].isin(recommended_movie_ids)]

print(f"\nRekomendasi untuk User ID {user_id}")
print("========================================")
print("Film dengan rating tertinggi dari user:")
top_watched = df_ratings[df_ratings['userId'] == user_id].sort_values(by='rating', ascending=False).head(5)
print(df_movies[df_movies['movieId'].isin(top_watched['movieId'])][['title', 'genres']])

print("\nTop 10 Rekomendasi Film:")
print(recommended_movies[['title', 'genres']])

"""## Content Based Filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

vectorizer_title = TfidfVectorizer(ngram_range=(1,2))
tfidf_title = vectorizer_title.fit_transform(df_movies['clean_title'])

vectorizer_genres = TfidfVectorizer(ngram_range=(1,2))
tfidf_genres = vectorizer_genres.fit_transform(df_movies['genres_list'])

def search_by_title(title, top_n=5):
    title = title.lower().strip()
    query_vec = vectorizer_title.transform([title])
    similarity = cosine_similarity(query_vec, tfidf_title).flatten()
    indices = np.argpartition(similarity, -top_n)[-top_n:]
    results = df_movies.iloc[indices].copy()
    results['similarity'] = similarity[indices]
    results = results.sort_values(by='similarity', ascending=False)
    return results

def search_similar_genres(genres, top_n=20):
    genres = genres.lower().strip()
    query_vec = vectorizer_genres.transform([genres])
    similarity = cosine_similarity(query_vec, tfidf_genres).flatten()
    indices = np.argpartition(similarity, -top_n)[-top_n:]
    results = df_movies.iloc[indices].copy()
    results['similarity'] = similarity[indices]
    results = results.sort_values(by='similarity', ascending=False)
    return results

def scores_calculator(movie_id):
    # Cari pengguna yang menyukai film ini
    similar_users = combined_data[
        (combined_data['movieId'] == movie_id) & (combined_data['rating'] >= 4)
    ]['userId'].unique()

    # Film yang disukai oleh user serupa
    similar_user_recs = combined_data[
        (combined_data['userId'].isin(similar_users)) & (combined_data['rating'] >= 4)
    ]['movieId'].value_counts()
    similar_user_recs = similar_user_recs / similar_user_recs.max()

    # Film populer secara umum
    all_users_recs = combined_data[
        (combined_data['movieId'].isin(similar_user_recs.index)) & (combined_data['rating'] >= 4)
    ]['movieId'].value_counts()
    all_users_recs = all_users_recs / all_users_recs.max()

    # Genre film yang dipilih
    genres_of_selected_movie = df_movies[df_movies['movieId'] == movie_id]['genres_list'].values[0]

    # Film dengan genre mirip
    movies_with_similar_genres = search_similar_genres(genres_of_selected_movie)
    genre_similar_ids = movies_with_similar_genres['movieId'].values

    # Bobot untuk genre similarity
    for idx in genre_similar_ids:
        if idx in similar_user_recs:
            similar_user_recs[idx] *= 1.5
        if idx in all_users_recs:
            all_users_recs[idx] *= 0.9

    # Gabungkan skor
    rec_percentages = pd.concat([similar_user_recs, all_users_recs], axis=1).fillna(0)
    rec_percentages.columns = ['similar', 'all']
    rec_percentages['score'] = 0.6 * rec_percentages['similar'] + 0.4 * rec_percentages['all']

    # Hitung cosine similarity genre (sim_score)
    sim_scores = {}
    for idx in rec_percentages.index:
        target_genres = df_movies[df_movies['movieId'] == idx]['genres_list'].values
        if len(target_genres) > 0:
            query_vec = vectorizer_genres.transform([genres_of_selected_movie])
            target_vec = vectorizer_genres.transform([target_genres[0]])
            sim = cosine_similarity(query_vec, target_vec)[0][0]
        else:
            sim = 0.0
        sim_scores[idx] = sim
    rec_percentages['sim_score'] = pd.Series(sim_scores)

    # Skor akhir
    rec_percentages['final_score'] = (
        0.7 * rec_percentages['score'] + 0.3 * rec_percentages['sim_score']
    )

    # Normalisasi final_score ke 0–1
    scaler = MinMaxScaler()
    rec_percentages['final_score'] = scaler.fit_transform(rec_percentages[['final_score']])

    rec_percentages = rec_percentages.sort_values('final_score', ascending=False)
    return rec_percentages

def recommendation_results(user_input, choice=0):
    title_candidates = search_by_title(user_input)
    if len(title_candidates) == 0:
        return pd.DataFrame()

    movie_id = title_candidates.iloc[choice]['movieId']
    scores = scores_calculator(movie_id)

    results = scores.head(10).merge(df_movies, left_index=True, right_on='movieId')[[
        'title', 'genres_list', 'score', 'sim_score', 'final_score']]
    results = results.rename(columns={'genres_list': 'genres'})
    return results

user_input = "Toy Story"
print("Are you looking for (please choose a number):")
candidates = search_by_title(user_input)
for i in range(min(5, len(candidates))):
    print(f"{i}: {candidates.iloc[i]['title']}")

choice = 3

if choice in range(len(candidates)):
    print("\nRecommendations based on your choice:")
    print(recommendation_results(user_input, choice))
else:
    print("Sorry, invalid choice!")

def calculate_precision_recall_at_k(recommended_movie_ids, hold_out_movie_ids, k):
    top_k_recommendations = recommended_movie_ids[:k]
    hold_out_set = set(hold_out_movie_ids)

    relevant_and_recommended = sum([1 for movie_id in top_k_recommendations if movie_id in hold_out_set])

    precision_at_k = relevant_and_recommended / k if k else 0.0
    recall_at_k = relevant_and_recommended / len(hold_out_set) if hold_out_set else 0.0

    return precision_at_k, recall_at_k

# Simulasikan user yang sama seperti sebelumnya
user_id = 123
k = 10

# 1. Dapatkan hasil rekomendasi
recs_df = recommendation_results("Toy Story", choice=3)
recommended_ids = df_movies[df_movies['title'].isin(recs_df['title'])]['movieId'].tolist()

# 2. Dapatkan film relevan (yang disukai user)
liked_movies = df_ratings[(df_ratings['userId'] == user_id) & (df_ratings['rating'] >= 4.0)]
hold_out_ids = liked_movies['movieId'].tolist()

# 3. Hitung Precision@K dan Recall@K
precision, recall = calculate_precision_recall_at_k(recommended_ids, hold_out_ids, k)

# 4. Cetak hasil
print(f"\nUser ID: {user_id}")
print(f"Recommended Movie IDs (top-{k}): {recommended_ids}")
print(f"Relevant Movie IDs (liked by user): {hold_out_ids}")
print(f"Precision@{k}: {precision:.4f}")
print(f"Recall@{k}: {recall:.4f}")